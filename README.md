## Mini Data Lake (DuckDB • Docker • Python)

### Cette série de travaux pratiques propose de construire pas à pas un environnement complet de traitement, d’analyse et de visualisation de données, en s’appuyant sur des outils modernes comme DuckDB, Docker et Python.

Chaque TP peut être réalisé de façon indépendante, mais l’ensemble suit une logique d’intégration progressive :

 - ingestion et stockage des données,

 - analyse exploratoire,

 - mise en place et industrialisation de pipelines,

 - exposition et visualisation des résultats,

 - simulation d’un mini data lake local et interactif.

#### Contexte

Vous jouez le rôle d’un data ingénieur chargé de structurer et fiabiliser les flux de données internes d’une entreprise. Votre mission consiste à :

 - concevoir des pipelines d’ingestion performants,

 - optimiser les traitements analytiques,

 - mettre à disposition des outils simples pour explorer et visualiser les données,

 - assurer une approche reproductible, modulaire et efficace.

#### Outils utilisés

DuckDB : moteur analytique léger et performant,

Pandas : manipulation et analyse de données,

Streamlit : visualisation et exploration interactive,

Docker & Docker Compose : déploiement reproductible et portable.